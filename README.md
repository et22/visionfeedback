# Vision-Language Model Feedback for Iteratively Improving Analyses & Plots Generated by an LLM
Here, I piloted the use of a vision-language model (llama3.2-vision) to provide feedback to an LLM (deepseek-r1:32b) analyzing and plotting data. 

I prompted deepseek-r1:32b to write Python code to do an exploratory data analysis on a dataset that I collected recently and to save figures from its analyses.

I tested various base prompts for conducting exploratory analyses, and the effects of adding feedback from a VLM on generated plots.

To reproduce all results, run the ```vla.ipynb``` notebook. 

Models are run locally with Ollama. 
# visionfeedback
