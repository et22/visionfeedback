# Improving LLM figure quality with feedback from a vision-language model. 

### Overview
One limitation with using an LLM agent to generate plots is that the agent lacks visual feedback about plots, so it cannot detect visual anomalies and poor formatting decisions. Here, I piloted the use of a vision-language model (llama3.2-vision) to provide feedback to an LLM (deepseek-r1:32b) analyzing and plotting data. 

I prompted deepseek-r1:32b to write Python code to do an exploratory data analysis on a dataset that I collected recently. The generated code was then automatically executed with ‘exec’, generating a figure that is saved to a file path specified in the prompt. This process was repeated until 20 figures had been successfully generated and saved. To address formatting issues in the generated igures, I added a vision-language model (llama3.2-vision) to the loop that was prompted to examine the figure, describe what it shows, and make some suggestions for improvements. The LLM was then prompted to improve the figure based on the suggestions. 

To reproduce all results, run the ```vla.ipynb``` notebook. 

Models are run locally with Ollama. 

### Results

<p align="center">
<img src="https://github.com/et22/visionfeedback/blob/main/figures/figure1.png" alt="Figure 1" width="900"/>
</p>

*Figure 1.* Example plot generated by LLM before and after feedback from a vision-language model.


<p align="center">
<img src="https://github.com/et22/visionfeedback/blob/main/figures/figure2.png" alt="Figure 2" width="900"/>
</p>

*Figure 2.* Example plot generated by LLM before and after feedback from a vision-language model.